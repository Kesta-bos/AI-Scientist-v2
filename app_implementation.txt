# AI Scientist v2 Gradio Demo Implementation Log

## Setup & Structure (app.py)
- Created `app.py`.
- Added project root to `sys.path` to allow importing `ai_scientist` modules.
- Imported necessary `gradio`, `os`, `sys`, `json`, `time`, `traceback`, `datetime`, `shutil`, `re`.
- Imported placeholder functions/modules from `ai_scientist` (will refine imports later).
- Added helper functions: `get_timestamp`, `format_log`.
- Created basic Gradio UI structure using `gr.Blocks` and `gr.Tabs`.
  - Tab 1: Input (Textbox, Dropdown, Button, Status Textbox).
  - Tab 2: Hypothesis (Log Textbox, Accordion for SemScholar, Final JSON). Increased log lines.
  - Tab 3: Experimentation (Stage Markdown, Log Textbox, Accordion for Plots/Gallery, Accordion for Best Node JSON).
  - Tab 4: Reporting (Accordions for Citations/Reflection/Compilation Logs, LaTeX Code, PDF File output).
- Defined the main orchestration function `run_ai_scientist_pipeline` as a generator.
- Connected the "Start" button (`start_button.click`) to `run_ai_scientist_pipeline`.
  - Specified all potentially updatable UI components in the `outputs` list.
  - Enabled Gradio's progress tracking (`show_progress="full"`).
- Added basic API key check on startup.
- Launched the demo using `demo.launch(debug=True)`.

## Phase 1: Ideation Integration (Actual Function Call)
- **Replaced** the simulation block for Phase 1.
- Imported `StringIO`, `contextlib`, `create_client`, `generate_temp_free_idea`.
- Added `parse_ideation_logs` helper function to separate general print logs from Semantic Scholar interactions based on patterns in stdout.
- In `run_ai_scientist_pipeline`:
  - Created `base_run_dir` using timestamp and part of the user prompt.
  - Defined `idea_json_path` within `base_run_dir`.
  - Set `max_generations=1` and `num_reflections=2` for faster demo execution.
  - Selected `ideation_model` (e.g., "gpt-4o-mini-2024-07-18").
  - Created the LLM client using `create_client`.
  - Used `contextlib.redirect_stdout` and `StringIO` to capture print output from `generate_temp_free_idea`.
  - Called `generate_temp_free_idea` with `reload_ideas=False`.
  - Parsed the captured stdout using `parse_ideation_logs`.
  - Yielded the parsed general logs to `ideation_log_textbox` and Semantic Scholar logs to `ideation_sem_scholar_textbox`.
  - Checked if ideas were generated. If yes, selected the first idea (`final_idea`) and yielded it to `ideation_final_json`.
  - If no ideas were generated or an exception occurred, yielded error messages and stopped the pipeline. Added specific check for `ImportError` related to `semantic_scholar`.
- **Note:** Capturing intermediate states beyond stdout parsing would require modifying `generate_temp_free_idea`.

## Phase 2: Experimentation Integration (Actual Function Call via Threading & Monitoring)
- **Replaced** the simulation block for Phase 2.
- Imported `threading`, `yaml`, `Path`.
- Imported `perform_experiments_bfts` and `edit_bfts_config_file`.
- Added `monitor_experiment_progress` helper function:
    - Takes the Gradio run directory and a (currently placeholder) callback function.
    - Defines expected log/result paths (`journal.jsonl`, `experiment_results`).
    - Enters a loop polling for changes:
        - Reads new lines from `journal.jsonl` since last read position.
        - Parses JSON log entries, extracts relevant info (stage, node, status, metrics, error).
        - Updates stage display via callback if stage changes.
        - Formats and yields log updates via callback.
        - Scans `experiment_results` recursively for new `.png` files based on modification time.
        - Updates plot gallery via callback if new plots found.
        - Includes a simple timeout/stall detection based on lack of file changes.
        - Includes check for a `.experiment_done` file as a completion signal.
    - **Note:** Direct yielding from the monitor thread to Gradio is problematic. Current implementation prints monitor updates; main thread yields based on monitor's *return* value. Real-time updates would need queues or Gradio API mode.
- In `run_ai_scientist_pipeline`:
    - Added yield to switch to Experimentation tab.
    - Prepared paths for base config, run-specific config, and final idea JSON.
    - Called `edit_bfts_config_file` to create a run-specific config pointing to the correct run directory and idea file.
    - Defined `experiment_runner` function to encapsulate the call to `perform_experiments_bfts` within a thread. Includes basic error handling and touches `.experiment_done` file on completion/error. Sets `AI_SCIENTIST_ROOT` env var for the thread.
    - Started `experiment_runner` in a `threading.Thread`.
    - Called `monitor_experiment_progress` to track the run.
    - **Removed** explicit `thread.join()` to avoid blocking Gradio UI. Relies on monitor timeout/completion signal.
    - Added check for errors reported back from the thread via `experiment_thread_result`.
    - Yielded final completion status.
    - Updated the subsequent (still simulated) Reporting phase to use the list of plots found by the monitor (`simulated_plots = monitor_result.get("plots", [])`).
- **Safety:** Acknowledged local execution risk without explicit sandboxing.

## Phase 3: Reporting Integration (Simulation - Minor Update)
- Kept the simulation logic for Phase 3.
- Updated simulated LaTeX `\graphicspath` to be relative to the `latex` subdirectory where compilation happens (`../logs/0-run/experiment_results/`).
- Changed dummy bibliography style to `plainnat` for better compatibility if `iclr2025_conference.bst` isn't copied correctly.
- Added code to *attempt* real LaTeX compilation within the simulation block:
    - Creates `latex` subdirectory.
    - Copies necessary `.sty`/`.bst` files from `blank_icbinb_latex`.
    - Writes generated LaTeX (`sim_latex`) and BibTeX (`sim_citations`) to files.
    - **Commented out** the actual call to `compile_latex` for now to keep this phase simulated.
    - Falls back to creating a dummy text file if real compilation is skipped or fails.
- **TODO:** Integrate actual Reporting phase calls (`gather_citations`, `perform_icbinb_writeup`). Uncomment and test `compile_latex`.

## Next Steps:
1.  **Test** the integrated Experimentation phase.
    - Requires a working local setup for the AI Scientist code execution (dependencies, potentially GPU).
    - Set appropriate API keys for models used in `bfts_config.yaml`.
    - Monitor the terminal output for messages from the experiment thread and the monitor thread.
    - Check the Gradio UI for log updates and plot appearances. Debug file paths and monitoring logic. Address any thread communication issues (currently minimal).
2.  Integrate the **Reporting phase** (`gather_citations`, `perform_icbinb_writeup`, `compile_latex`). Ensure LaTeX compilation works locally.